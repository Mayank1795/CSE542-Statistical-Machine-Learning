{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_images(dataPath):\n",
    "    \"\"\"\n",
    "        Dataset 1 : Resize all the images two 48 x 42 from 192 x 168.\n",
    "    \"\"\"\n",
    "    every_path = list(os.walk(dataPath))\n",
    "    data = []\n",
    "    \n",
    "    for i in range(0,len(every_path)):\n",
    "        dirPath, dirName, fileNames = every_path[i]\n",
    "        if(len(dirName) == 0):\n",
    "            for j in fileNames:\n",
    "                single_doc = []\n",
    "                single_doc_loc = dirPath + '/' + j\n",
    "                img = io.imread(single_doc_loc)\n",
    "                \n",
    "                if(img.shape[0] == 192 and img.shape[1] == 168): # removing images with other dim, there are some.\n",
    "                    img_re = resize(img, (48, 42), anti_aliasing = True)\n",
    "                    img_re = img_re.flatten().tolist()\n",
    "                    img_re.append(dirPath.split('/')[-1])\n",
    "                    data.append(img_re)\n",
    "    \n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(dataPath):\n",
    "    \"\"\"\n",
    "        Dataset 2\n",
    "    \"\"\"\n",
    "    every_path = list(os.walk(dataPath))\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    meta = []\n",
    "    \n",
    "    for i in range(0,len(every_path)):\n",
    "        dirPath, dirName, fileNames = every_path[i]\n",
    "        folder = dirPath.split('/')[-1]\n",
    "        \n",
    "        if(len(dirName) == 0):\n",
    "            for j in fileNames:\n",
    "                single_doc_loc = dirPath + '/' + j\n",
    "                \n",
    "                with open(single_doc_loc, 'rb') as fo:\n",
    "                    dt = pickle.load(fo, encoding='bytes')\n",
    "                    \n",
    "                    if(folder == 'train'):\n",
    "                        train_data.append(decoding(dt))\n",
    "                    elif(folder == 'test'):\n",
    "                        test_data.append(decoding(dt))\n",
    "                    elif(folder == 'meta'):\n",
    "                        meta.append(decoding(dt))\n",
    "                        \n",
    "    return train_data, test_data, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(d):\n",
    "    \n",
    "    datapt = {}\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, bytes):\n",
    "            datapt[k.decode(\"utf-8\")] = v.decode(\"utf-8\")          \n",
    "        else:\n",
    "            datapt[k.decode(\"utf-8\")] = v\n",
    "            \n",
    "    return datapt           \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataMatrix(data, n):\n",
    "    \"\"\" Dataset 1 \"\"\"\n",
    "    mat = []\n",
    "    \n",
    "    # Handle the case  with unbalanced data - to be done.\n",
    "    \n",
    "    # Balanced data set\n",
    "    total_images = 0\n",
    "    for k in n.keys():\n",
    "        total_images+= n[k]\n",
    "    \n",
    "    print('total valid images:', total_images)\n",
    "    \n",
    "    c = len(n.keys()) # no. of class\n",
    "    \n",
    "    for i in range(0, total_images):\n",
    "        mat.append(data[i].flatten().tolist())\n",
    "    \n",
    "    matrix = np.array(mat, dtype='uint8')\n",
    "    return matrix\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataMatrix(imgs):\n",
    "    \"\"\" Dataset 2 \"\"\"\n",
    "    data_mat = []\n",
    "    data_label = []\n",
    "    \n",
    "    for batch in imgs:\n",
    "        for i in range(0, batch['data'].shape[0]):\n",
    "            data_mat.append(rgbToGray(batch['data'][i,:]).tolist())\n",
    "            data_label.append(batch['labels'][i])\n",
    "    \n",
    "    labels = np.array(data_label)[:,np.newaxis]\n",
    "    data_mat = np.append(data_mat, labels, axis=1)\n",
    "    \n",
    "    return data_mat\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data, req_energy_per):\n",
    "    \n",
    "    mean_sub = data - data.mean(axis = 0)\n",
    "\n",
    "    mat_cv = np.cov(data, rowvar = False)\n",
    "\n",
    "    eg_val, eg_vec = np.linalg.eig(mat_cv) #ignore imaginary part in small eigen values, due to numerical error\n",
    "\n",
    "    eg_val = eg_val.real\n",
    "    sorted_eg_val = sorted(eg_val.real, reverse = True)\n",
    "    \n",
    "#     print(sorted_eg_val)\n",
    "    cnt = 0\n",
    "    current_energy = 0\n",
    "    \n",
    "    energy_req = req_energy_per * sum(sorted_eg_val)\n",
    "    \n",
    "    for k in sorted_eg_val:\n",
    "        if(current_energy >= energy_req):\n",
    "            break\n",
    "        current_energy+=k\n",
    "        cnt+=1\n",
    "        \n",
    "    vals,vecs = np.linalg.eig(mat_cv)\n",
    "    vals = vals[vals.imag == 0]\n",
    "    vals = vals.real\n",
    "    \n",
    "    sorted_vals_pos = np.argsort(vals)[::-1][:cnt]\n",
    "    \n",
    "    top_vec = []\n",
    "    for s in sorted_vals_pos:\n",
    "        top_vec.append(vecs[:,s].tolist())\n",
    "    \n",
    "    vecs = np.array(top_vec)\n",
    "\n",
    "    lower_dim_pts = np.dot(vecs, data.T).T\n",
    "            \n",
    "    return lower_dim_pts, vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viewImg(imgArr, gray = False):\n",
    "    if(not gray):\n",
    "        io.imshow(imgArr)\n",
    "    else:\n",
    "        io.imshow(imgArr, cmap=\"gray\")\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgbToGray(img):\n",
    "    \"\"\"\n",
    "        Assuming rgb values are in row major order\n",
    "        Using: Y = 0.2125 R + 0.7154 G + 0.0721 B\n",
    "    \"\"\"\n",
    "  \n",
    "    img = np.reshape(img, (3, 1024)) # 32*32 = 1024 features\n",
    "    new_img = []\n",
    "    \n",
    "    for j in range(0, img.shape[1]):\n",
    "        gray_pixel = round(0.2125*img[0,j] + 0.7154*img[1,j] + 0.0721*img[2,j])\n",
    "        new_img.append(gray_pixel)\n",
    "        \n",
    "    img_gray = np.array(new_img)\n",
    "   \n",
    "    # viewImg(np.reshape(img_gray, (32,32)), True)\n",
    "    \n",
    "    return img_gray\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickling(data, file):\n",
    "    pk = open(file+'.pickle', 'wb')\n",
    "    pickle.dump(data, pk)\n",
    "    pk.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    pk = open(file+'.pickle', 'rb')\n",
    "    return pickle.load(pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startKFold(train_data, K, b = False, bag = False):\n",
    "    \"\"\"\n",
    "        Args: training data, K = no. of folds\n",
    "        returns: best trained classifier\n",
    "    \"\"\"\n",
    "    global no_of_iter # for adaboost\n",
    "    \n",
    "    all_sets = np.split(train_data, K, axis=0)\n",
    "\n",
    "    accuracy = []\n",
    "    errorate = []\n",
    "    model = []\n",
    "    for i in range(0, K):\n",
    "        \n",
    "        v_label = all_sets[i][:,-1] # Validation test labels\n",
    "        v_set = all_sets[i][:,:-1] # Validation test set\n",
    "        \n",
    "        train_si = []\n",
    "\n",
    "        train_si = [ j for j in range(0, K) if(j!=i)]\n",
    "        train_set = all_sets[train_si[0]]\n",
    "        \n",
    "        for j in range(1, len(train_si)):\n",
    "            train_set = np.concatenate((train_set, all_sets[train_si[j]]), axis=0)\n",
    "            \n",
    "        if(b):\n",
    "            # have to convert to pandas from np array\n",
    "            train_Acc, test_Acc = Adaboost(no_of_iter, pd.DataFrame(train_set), pd.DataFrame(all_sets[i]))\n",
    "            print('Cross Validation Fold ',i,' test set accuracy is :', test_Acc)\n",
    "            accuracy.append(train_Acc)\n",
    "            errorate.append(100 - train_Acc)\n",
    "        \n",
    "        elif(bag):\n",
    "             # have to convert to pandas from np array\n",
    "            train_Acc, test_Acc = bagging(no_of_iter, pd.DataFrame(train_set), pd.DataFrame(all_sets[i])) # no normalisation\n",
    "            print('Cross Validation Fold ',i,' test set accuracy is :', test_Acc)\n",
    "            accuracy.append(train_Acc)\n",
    "            errorate.append(100 - train_Acc)\n",
    "        \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            t_label = train_set[:,-1] # Validation training labels\n",
    "            t_set = train_set[:, :-1] # Validation training set\n",
    "            \n",
    "            classifier = GaussianNB()\n",
    "            classifier.fit(t_set, t_label)\n",
    "            model.append(classifier)\n",
    "            accuracy.append(classifier.score(v_set, v_label))\n",
    "    \n",
    "    if(b):\n",
    "        print('Mean accuracy (boosting): ', stat.mean(accuracy),'%', 'Standard deviation of accuracy: ',stat.pstdev(accuracy))\n",
    "        print('Mean of error rate: ', stat.mean(errorate),'%', ' Standard deviation of error rate: ',stat.pstdev(errorate))\n",
    "        return\n",
    "    \n",
    "    elif(bag):\n",
    "        print('Mean accuracy(bagging): ', stat.mean(accuracy),'%', 'Standard deviation of accuracy: ',stat.pstdev(accuracy))\n",
    "        print('Mean of error rate: ', stat.mean(errorate),'%', ' Standard deviation of error rate: ',stat.pstdev(errorate))\n",
    "        return        \n",
    "        \n",
    "    else:\n",
    "        print('Mean of accuracy: ', stat.mean(accuracy), 'Standard deviation of accuracy: ',stat.pstdev(accuracy))\n",
    "        best_model_pos = accuracy.index(max(accuracy))\n",
    "        return model[best_model_pos]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(train_set):\n",
    "    \"\"\"\n",
    "        Args: training set (in x feature space) last column class labels are integers\n",
    "        Output: Projected set (in y feature space) Y = W^t . X\n",
    "    \"\"\"\n",
    "    ts = pd.DataFrame(train_set)\n",
    "    ts.rename(columns = {ts.columns[-1] : \"class\"}, inplace = True)\n",
    "   \n",
    "    # No. of points in class [Ni]\n",
    "    unique, counts = np.unique(train_set[:,-1], return_counts=True)\n",
    "   \n",
    "    # Classes labels [wi] and their respective counts\n",
    "    w = unique.tolist()\n",
    "    N = counts.tolist()\n",
    "    \n",
    "    data_sep_classwise = {}\n",
    "    \n",
    "    for i in w:\n",
    "        # ith class\n",
    "        data_sep_classwise[i] = (ts.loc[ts['class'] == i])\n",
    "    \n",
    "    # Compute mean of every class i [0 - 9] = [1/Ni sum(x belonging to class wi)]\n",
    "    mean = {}\n",
    "    for i,v in data_sep_classwise.items():\n",
    "        mean[int(i)] = v.iloc[:,:-1].mean(axis = 0)       \n",
    "        \n",
    "    # Overall mean [1/N sum(all x)]\n",
    "    overall_mean = ts.iloc[:, :-1].mean(axis = 0)\n",
    "    \n",
    "    # Sum of cov. matrices of each class Si = Sw [withing class scatter matrix of the data in x feature space]\n",
    "    f = v.iloc[:,:-1].shape[1] # no. of features\n",
    "    Sw = pd.DataFrame(np.zeros((f,f)))\n",
    "    \n",
    "    for i,v in data_sep_classwise.items():\n",
    "        if(w[0]!=0):\n",
    "            Sw+=(v.iloc[:,:-1].cov()) #*(N[int(i)-1]-1)\n",
    "        else:\n",
    "            Sw+=(v.iloc[:,:-1].cov()) #*(N[int(i)]-1)          \n",
    "            \n",
    "   \n",
    "    # Compute between class scatter in the x feature space [sum(over classes) [Ni (ui -u)(ui-ui)^T]]\n",
    "    Sb = pd.DataFrame(np.zeros((f,f)))\n",
    "    \n",
    "    for i,m in mean.items():\n",
    "        term = (m - overall_mean).to_frame()\n",
    "        if(w[0]!=0):\n",
    "            Sb+=N[int(i)-1]*(term.dot(term.T))\n",
    "        else:\n",
    "            Sb+=N[int(i)]*(term.dot(term.T))\n",
    "    \n",
    "    # Compute Sw^-1. Sb\n",
    "    W = np.dot(np.linalg.inv(Sw.to_numpy()), Sb)\n",
    "    \n",
    "    \n",
    "    # eigen vectors and eigen values, of Sw^-1. Sb eigen vectors are projection vectors\n",
    "#     vals, vecs = eigs(W, k = len(w)-1, which = 'LR', maxiter = 30000)\n",
    "    vals,vecs = np.linalg.eig(W)\n",
    "    vals = vals[vals.imag == 0]\n",
    "    vals = vals.real\n",
    "    \n",
    "    sorted_vals_pos = np.argsort(vals)[::-1][:len(w)-1]\n",
    "    \n",
    "    top_vec = []\n",
    "    for s in sorted_vals_pos:\n",
    "        top_vec.append(vecs[:,s].tolist())\n",
    "    \n",
    "    vecs = np.array(top_vec)\n",
    "    \n",
    "    lda_space_pt = np.dot(vecs, train_set[:,:-1].T).T  \n",
    "    lda_space = np.append(lda_space_pt, train_set[:,-1][:,np.newaxis], axis = 1)\n",
    "    \n",
    "    return lda_space, vecs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(train_data, test_data):\n",
    "    \"\"\"\n",
    "        1. Train Naive Bayes\n",
    "        2. Find out accuracy on Test data.\n",
    "    \"\"\"\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(train_data[:,:-1], train_data[:,-1])\n",
    "    print('Accuracy: ', classifier.score(test_data[:,:-1], test_data[:,-1]))\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidation(data, K, b = False, bag = False):\n",
    "    np.random.shuffle(data)\n",
    "    return startKFold(data, K, b, bag)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyTestset(model, test_d):\n",
    "    \"\"\"\n",
    "        Args: best trained model\n",
    "        output: Accuracy, Confusion matrix\n",
    "    \"\"\"\n",
    "    print('Accuracy using best model on Test set: ', model.score(test_d[:,:-1], test_d[:,-1]))\n",
    "    cm = np.zeros((no_of_classes, no_of_classes))\n",
    "    \n",
    "    predicted_cls = model.predict(test_d[:,:-1])\n",
    "    \n",
    "    for i in range(0, len(predicted_cls)):\n",
    "        cm[int(predicted_cls[i]), int(test_d[i,-1])]+=1\n",
    "        \n",
    "    return cm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomSplitDataset(nr, a):\n",
    "    \"\"\"\n",
    "        Args: a = percentage of data points for training set\n",
    "    \"\"\"\n",
    "    no_train_rows = round((a*nr)/100)    \n",
    "    return no_train_rows\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSplit(nr, K):\n",
    "    req_rows = nr - (nr % K)\n",
    "    return req_rows\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_letters(path):\n",
    "    data = pd.read_csv(path+'/letter-recognition.data', header = None)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def superClassifier(h_alpha, data):\n",
    "    \"\"\"\n",
    "        h_alpha : all the alphas with prediction of every weak classifier for all the data\n",
    "        data : find the accuracy on this data. With true labels given in last column.\n",
    "    \"\"\"\n",
    "    \n",
    "    match = 0\n",
    "    \n",
    "    for i in range(0, data.shape[0]):\n",
    "#         clear_output(wait = True)\n",
    "        predicted_label = h_alpha.groupby([i+1])[[0]].sum()[0].idxmax()\n",
    "        predicted_label = int(predicted_label)\n",
    "        if(predicted_label == data.iloc[i,-1]):\n",
    "            match+=1\n",
    "#         print('Current progress: ', np.round((i/data.shape[0])*100,2))\n",
    " \n",
    "    return np.round((match/data.shape[0])*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adaboost(N, train_data, test_data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        - N : no. of rounds of boosting\n",
    "        - Train : Training set\n",
    "        - Test : Testing set\n",
    "    Returns:\n",
    "        - Train & test acc.\n",
    "        \n",
    "    Using decision tree upto 2 levels of tree and 5 nodes.\n",
    "    \n",
    "    \"\"\"\n",
    "    nd_train = train_data.shape[0]\n",
    "    nd_test = test_data.shape[0]\n",
    "    \n",
    "    nc = len(alphabet.keys()) - 1\n",
    "    \n",
    "    ar = np.zeros((N, nd_train + 1))  # each row : alpha of kth clf | its predictions\n",
    "    ar1 = np.zeros((N, nd_test + 1))\n",
    "    \n",
    "    H_alpha_train = pd.DataFrame(data=ar)\n",
    "    H_alpha_test = pd.DataFrame(data=ar1)\n",
    "\n",
    "    arr = np.zeros((nd_train, 3))\n",
    "    \n",
    "    df = pd.DataFrame(data= arr, columns = ['weight', 'tlabel', 'plabel'])\n",
    "    df['weight'] = 1/nd_train\n",
    "    df['tlabel'] = train_data.iloc[:, -1]\n",
    "    \n",
    "    # AdaBoost\n",
    "    \n",
    "    for k in range(0, N):\n",
    "    \n",
    "        # call weak learner - minimizes error approx. of train_data with weights as input.\n",
    "        \n",
    "        dt = DecisionTreeClassifier(max_depth = 2, max_leaf_nodes = 3)\n",
    " \n",
    "        dt.fit(train_data.iloc[:,:-1], df['tlabel'], sample_weight = df['weight'].values)\n",
    "        \n",
    "        predictions_train = dt.predict(train_data.iloc[:,:-1])\n",
    "        predictions_test = dt.predict(test_data.iloc[:,:-1])\n",
    "        \n",
    "        df['plabel'] = predictions_train\n",
    "        \n",
    "        # training error\n",
    "        eps = df.loc[df['plabel'] != df['tlabel'], 'weight'].sum()/df['weight'].sum()\n",
    "        \n",
    "        alpha = np.log((1-eps)/eps) + np.log(nc)  # Learning Rate 1, SAMME\n",
    "        \n",
    "        if(eps <= (1 - 1/nd_train)):\n",
    "\n",
    "            df.loc[df['tlabel'] != df['plabel'], 'weight']*= np.exp(alpha)\n",
    "            df['weight']/=df['weight'].sum()\n",
    "            H_alpha_train.loc[k] = [alpha] + predictions_train.tolist()  \n",
    "            H_alpha_test.loc[k] = [alpha] + predictions_test.tolist()  \n",
    "            \n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    train_acc = superClassifier(H_alpha_train, train_data)\n",
    "    test_acc = superClassifier(H_alpha_test, test_data)\n",
    "                    \n",
    "    return train_acc, test_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baggingClassifier(h, data):\n",
    "    \n",
    "    predicted_labels = np.argmax(h, axis=1) \n",
    "    acc = accuracy_score(predicted_labels, data.iloc[:,-1])*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(N, train_data, test_data, norm = False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        - N : no. of rounds of boosting\n",
    "        - Train : Training set\n",
    "        - Test : Testing set\n",
    "    Returns:\n",
    "        - Train & test acc.\n",
    "        \n",
    "    Using decision tree upto 2 levels of tree and 5 nodes.\n",
    "    \n",
    "    \"\"\"\n",
    "    nd_train = train_data.shape[0]\n",
    "    nd_test = test_data.shape[0]\n",
    "    \n",
    "    nc = len(alphabet.keys())\n",
    "    \n",
    "    H_train = np.zeros((nd_train, nc))\n",
    "    H_test = np.zeros((nd_test, nc))\n",
    "\n",
    "    h_train = np.zeros((N, nd_train), dtype='int64')\n",
    "    h_test = np.zeros((N, nd_test), dtype ='int64')\n",
    "    \n",
    "    for k in range(0, N):\n",
    "        \n",
    "        tdata = train_data.sample(n=train_data.shape[0], replace = True).reset_index(drop = True)\n",
    "        \n",
    "        dt = DecisionTreeClassifier(max_depth = 2, max_leaf_nodes = 3)\n",
    " \n",
    "        dt.fit(tdata.iloc[:,:-1], tdata.iloc[:, -1])\n",
    "        \n",
    "        score_train = dt.predict_proba(train_data.iloc[:,:-1])\n",
    "        score_test = dt.predict_proba(test_data.iloc[:,:-1])\n",
    "        \n",
    "        if(norm == 'tanh'):\n",
    "            # Normalise using min max/ tanh/ zscore\n",
    "            score_train = np.tanh(score_train)\n",
    "            score_test = np.tanh(score_test)\n",
    "                  # Sums of scores\n",
    "            H_train+=score_train\n",
    "            H_test+=score_test\n",
    "            \n",
    "        elif(norm == 'minmax'):\n",
    "            sc = MinMaxScaler(feature_range=(0, 1), copy = False)\n",
    "            sc = sc.fit(score_train)\n",
    "            sc.transform(score_train)\n",
    "            \n",
    "            sc = MinMaxScaler(feature_range=(0, 1), copy = False)\n",
    "            sc = sc.fit(score_test)\n",
    "            sc.transform(score_test)\n",
    "              # Sums of scores\n",
    "            H_train+=score_train\n",
    "            H_test+=score_test\n",
    "            \n",
    "        elif(norm == 'zscore'):\n",
    "            score_train = zscore(score_train)\n",
    "            score_test = zscore(score_test)\n",
    "                  # Sums of scores\n",
    "            H_train+=score_train\n",
    "            H_test+=score_test\n",
    "            \n",
    "        else:\n",
    "            predictions_train = dt.predict(train_data.iloc[:,:-1])\n",
    "            predictions_test = dt.predict(test_data.iloc[:,:-1])\n",
    "            h_train[k,:] = predictions_train.tolist()  \n",
    "            h_test[k,:] = predictions_test.tolist()  \n",
    "           \n",
    "            \n",
    "                \n",
    "    if(not norm):\n",
    "        train_acc = baggingClassifier2(h_train, train_data)\n",
    "        test_acc = baggingClassifier2(h_test, test_data)\n",
    "    else:\n",
    "        train_acc = baggingClassifier(H_train, train_data)\n",
    "        test_acc = baggingClassifier(H_test, test_data)\n",
    "                    \n",
    "    return train_acc, test_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baggingClassifier2(h ,data):\n",
    "    predictions = []\n",
    "    for i in range(0, data.shape[0]):\n",
    "#         print(type(np.bincount(h[:,i])))\n",
    "        predictions.append(np.bincount(h[:,i]).argmax())\n",
    "        \n",
    "    acc = accuracy_score(predictions, data.iloc[:,-1])*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
